library(twitteR)
library(wordcloud)
library(RColorBrewer)
library(tm)
consumer_key <- "7D66e6j5YOXb1hOKBxqFqiGth"
consumer_secret <- "rKU47DiEDN8vR72lp950xAVH7B0SvMZ63MiTuVmc8V4ComsvG5"
access_token <- "214399148-A8sIkbFjmkKmefiEnqJzlvp2ighgpfnRqqjaF65P"
access_secret <- "y5pgnu5FoRvWyuM1MTvD1jB6exhNUBFwHXaSiax7lZC2P"
setup_twitter_oauth(consumer_key ,consumer_secret,access_token ,access_secret)
tweets <- twitteR::searchTwitter("#IPL",n =1000,lang ="en",since = '2018-04-01')
df <- twListToDF(tweets)
saveRDS(df, file="mytweets.rds")
df2 <- readRDS("mytweets.rds")
newdata <- iconv(df2$text, "ASCII", "UTF-8", sub="")
mydata <- Corpus(VectorSource(df2))
mydata <- tm_map(mydata, content_transformer(tolower))
mydata<-tm_map(mydata, content_transformer(gsub), pattern="\\W",replace=" ")
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeURL))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
mydata <- tm_map(mydata, removeWords, stopwords("english"))
mydata <- tm_map(mydata, stripWhitespace)
mydata <- tm_map(mydata, removeNumbers)
mydata <- tm_map(mydata, removePunctuation)
mydataCopy <- mydata
dtm <- TermDocumentMatrix(mydata)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 20)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 10, max.words = 500, random.order=FALSE, scale = c(3, 0.5), colors=rainbow(50))
View(d)
install_github('TextRegression', username = 'johnmyleswhite')
library(devtools)
install_github('TextRegression', username = 'johnmyleswhite')
install_github('TextRegression', username = 'johnmyleswhite')
library(TextRegression)
text <- c('saying text is good',
'saying text once and saying text twice is better',
'saying text text text is best',
'saying text once is still ok',
'not saying it at all is bad',
'because text is a good thing',
'we all like text',
'even though sometimes it is missing')
y <- c(1, 2, 3, 1, 0, 1, 1, 0)
results <- regress.text(text,y)
detach("package:tm", unload=TRUE)
library("TextRegression", lib.loc="~/R/win-library/3.5")
detach("package:TextRegression", unload=TRUE)
remove.packages("TextRegression", lib="~/R/win-library/3.5")
library(java)
install.packages("java")
install.packages("NLP")
install.packages("openNLP")
install.packages("RWeka")
install.packages("qdap")
library(NLP)
library(openNLP)
library(RWeka)
library(java)
library(rJava)
library(NLP)
library(OpenNLP)
library(openNLP)
library(RWeka)
setwd("~/R/Naive Bayes")
#Importing Dataset
dataset = read.csv('dataset.csv')
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset$news))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
#Bag of words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset1 = as.data.frame(as.matrix(dtm))
dataset1$headline_type = dataset$type
#splitting data into training and test data
#install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset1$headline_type, SplitRatio = 0.75)
test_set = subset(dataset1, split == TRUE)
training_set = subset(dataset1, split == FALSE)
#Fitting Naive Bayes to the training set
#install.packages('e1071')
library(e1071)
classifier = naiveBayes(x = training_set[-8894],
y = training_set$headline_type)
#Predicting the Test set Results
y_pred = predict(classifier, newdata = test_set[-8894])
#Making the COnfusion Matrix
cm = table(test_set[, 8894], y_pred)
cm
classifier$apriori
#Finding Accuracy
accuracy = (cm[1,1] + cm[2,2] + cm[3,3] + cm[4,4] + cm[5,5]) / (cm[1,1]+cm[1,2]+cm[1,3]+cm[1,4]+cm[1,5]+cm[2,1]+cm[2,2]+cm[2,3]+cm[2,4]+cm[2,5]+cm[3,1]+cm[3,2]+cm[3,3]+cm[3,4]+cm[3,5]+cm[4,1]+cm[4,2]+cm[4,3]+cm[4,4]+cm[4,5]+cm[5,1]+cm[5,2]+cm[5,3]+cm[5,4]+cm[5,5])
print(accuracy)
library(plotly)
library(ggplot2)
